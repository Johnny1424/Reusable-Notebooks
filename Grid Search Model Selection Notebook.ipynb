{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Model Selection Notebook\n",
    "\n",
    "***\n",
    "\n",
    "The purpose of this notebook is to replicate some of the model selection functionality offered by DataRobot for client projects that cannot justify the sometimes prohibative cost of DataRobot implemetations\n",
    "\n",
    "## How to use\n",
    "\n",
    "The notebook guides you through the process of finding a good classifier and tuning the parameters with a grid search\n",
    "By default the classifiers that are evaluated are:\n",
    "\n",
    "    ExtraTrees\n",
    "    RandomForest\n",
    "    AdaBoost\n",
    "    GradientBoosting\n",
    "    XGBoostClassifier\n",
    "    \n",
    "These defaults also include some hyperparameter suggestions in the 'params' dictionary - these can be added/removed if you have some intuition about the kind of classifier that would do well for your scenario\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "1. Change the csv location etc : [Input csv Link](#quickstart_link)\n",
    "\n",
    "2. Change the name of the output filenames: [Filename Link](#quickstart_link2)\n",
    "\n",
    "3. Run all the cells in this notbook \n",
    "\n",
    "## Dependancies\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- sklearn\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Function\n",
    "\n",
    "Firstly we will define a helper function that will be used later to evaluate different models\n",
    "Dont worry too much about this function - the rest of the code in this workbook is explained in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset import and processing <a id='quickstart_link'></a>\n",
    "\n",
    "Next we will import the dataset we will be working on - in the default example the Nokia dataset is used because this document was originally prepared for use on the Nokia Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset here\n",
    "data = pd.read_csv(\"raw_dataset_edited_cont.csv\")\n",
    "\n",
    "# split the dataset into a features and target dataframe - here the target column is called 'Request Status'\n",
    "request_raw = data['REQUEST STATUS']\n",
    "features_raw = data.drop('REQUEST STATUS', axis = 1)\n",
    "\n",
    "# Encode the target dataframe to numerical values if required\n",
    "request = (request_raw =='APPROVED').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scaling & Normalisation - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Section allowing the dataset to be scaled - may require pipelines\n",
    "\n",
    "#skewed = ['CBMs', 'Data1', 'Data2', 'Data3', 'Weights(kgs)']\n",
    "#features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "#features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "#scaler = MinMaxScaler() # default=(0, 1)\n",
    "#numerical = ['CBMs', 'Data1', 'Data2', 'Data3', 'Weights(kgs)']\n",
    "\n",
    "#features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "#features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "#display(features_log_minmax_transform.head(n = 5))\n",
    "\n",
    "# Done: One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "\n",
    "#features_final = pd.get_dummies(features_log_minmax_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the necessary columns in the features dataframe\n",
    "features_final = pd.get_dummies(features_raw)\n",
    "\n",
    "# Print the total number of features after one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split datasets into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the features and target data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    request, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define models of interest and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly import the classifiers that are of interest\n",
    "# the classifiers included below should  be good for most situations but feel free to add any others\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add any classifiers you want to evaluate into this dictionary\n",
    "\n",
    "models1 = {\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'XGBoostClassifier': XGBClassifier()\n",
    "}\n",
    "\n",
    "# For each classifier above add the hyperparameters you would like to evaluate\n",
    "# The defaults included included below should cover most situation\n",
    "\n",
    "params1 = {\n",
    "    'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n",
    "    'RandomForestClassifier': {'min_samples_split' : [2,4,6,8,10,14,20], 'n_estimators': [1, 5, 10, 15, 20]},\n",
    "    'AdaBoostClassifier':  { 'n_estimators': [16, 32] },\n",
    "    'GradientBoostingClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
    "    'XGBoostClassifier': {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Grid Search evaluation\n",
    "\n",
    "This step can take some time depending on the dataset and number of model combinations defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ExtraTreesClassifier.\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 3 folds for each of 35 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 105 out of 105 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   6 out of   6 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  12 out of  12 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for XGBoostClassifier.\n",
      "Fitting 3 folds for each of 405 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=2)]: Done 1215 out of 1215 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(X_train, y_train, scoring='f1', n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Search Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.878986</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.0148519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.844106</td>\n",
       "      <td>0.866888</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.0216532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>0.870222</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.0150067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.876521</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.0140179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>XGBoostClassifier</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.869137</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.0146966</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.872072</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.0146086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.86166</td>\n",
       "      <td>0.873439</td>\n",
       "      <td>0.887967</td>\n",
       "      <td>0.0109142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.868106</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.0135242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>XGBoostClassifier</td>\n",
       "      <td>0.817121</td>\n",
       "      <td>0.854978</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.0286106</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.866213</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.0149063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      estimator min_score mean_score max_score  std_score  \\\n",
       "6        RandomForestClassifier  0.860465   0.878986  0.896825  0.0148519   \n",
       "29       RandomForestClassifier  0.844106   0.866888     0.896  0.0216532   \n",
       "21       RandomForestClassifier  0.854962   0.870222  0.890625  0.0150067   \n",
       "1          ExtraTreesClassifier  0.857143   0.876521  0.889831  0.0140179   \n",
       "135           XGBoostClassifier  0.853659   0.869137  0.888889  0.0146966   \n",
       "10       RandomForestClassifier  0.852713   0.872072     0.888  0.0146086   \n",
       "40   GradientBoostingClassifier   0.86166   0.873439  0.887967  0.0109142   \n",
       "16       RandomForestClassifier  0.853846   0.868106  0.886275  0.0135242   \n",
       "401           XGBoostClassifier  0.817121   0.854978  0.886275  0.0286106   \n",
       "26       RandomForestClassifier  0.850575   0.866213  0.886275  0.0149063   \n",
       "\n",
       "    colsample_bytree gamma learning_rate max_depth min_child_weight  \\\n",
       "6                NaN   NaN           NaN       NaN              NaN   \n",
       "29               NaN   NaN           NaN       NaN              NaN   \n",
       "21               NaN   NaN           NaN       NaN              NaN   \n",
       "1                NaN   NaN           NaN       NaN              NaN   \n",
       "135              0.6     2           NaN         4                1   \n",
       "10               NaN   NaN           NaN       NaN              NaN   \n",
       "40               NaN   NaN           0.8       NaN              NaN   \n",
       "16               NaN   NaN           NaN       NaN              NaN   \n",
       "401                1     2           NaN         3               10   \n",
       "26               NaN   NaN           NaN       NaN              NaN   \n",
       "\n",
       "    min_samples_split n_estimators subsample  \n",
       "6                   2           20       NaN  \n",
       "29                 14           10       NaN  \n",
       "21                  8           20       NaN  \n",
       "1                 NaN           32       NaN  \n",
       "135               NaN          NaN         1  \n",
       "10                  4           15       NaN  \n",
       "40                NaN           32       NaN  \n",
       "16                  6           20       NaN  \n",
       "401               NaN          NaN       0.8  \n",
       "26                 10           20       NaN  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary view of the top 10 models\n",
    "helper1.score_summary(sort_by='max_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get best model or manually select below\n",
    "model_index = helper1.score_summary(sort_by='max_score').index[0]\n",
    "#model_index = 1 # Uncomment this line to select a model that isnt the top scorer\n",
    "\n",
    "selected_model = helper1.score_summary().iloc[helper1.score_summary().index == model_index]\n",
    "\n",
    "best_classifier = models1[selected_model.to_dict('records')[0]['estimator']]\n",
    "\n",
    "parameters = selected_model.drop(['estimator', 'min_score', 'mean_score', 'max_score', 'std_score'], axis=1).dropna(axis=1, how='all').to_dict('records')[0]\n",
    "\n",
    "best_classifier.set_params(**parameters)\n",
    "\n",
    "best_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy of the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79508196721311475"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pickle model <a id='quickstart_link2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nokia_model_v1_columns.pkl']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib is better than pickle when using large numpy arrays\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Choose where to save the file\n",
    "joblib.dump(model, 'Nokia_model_v1.pkl')\n",
    "joblib.dump(X_train.columns, 'Nokia_model_v1_columns.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Use with BP\n",
    "\n",
    "You now have a trained model saved to disk - this model can be used in a RPA implementation\n",
    "There is a BP object that will handle running the model as a webservice in the background - you just have to specify where the models you want to use are and pass a collection of features to get predictions - full details here - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
